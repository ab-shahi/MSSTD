Introduction

In the realm of naval surveillance, monitoring maritime environments plays a crucial role in ensuring coastal security and situational awareness. Traditionally, visual or electro-optical sensors are deployed to observe sea regions, enabling the detection of threats, intrusions, or anomalies. However, manual interpretation of such data—especially from vast amounts of video or imagery—is often labor-intensive, time-consuming, and susceptible to errors, particularly under challenging conditions like poor visibility or cluttered scenes.

This project focuses on developing an intelligent, deep learning-based system for multi-class semantic segmentation of maritime scenes. The aim is to automatically segment and classify different regions such as land, sky, and sea from visual inputs. A key objective is to further isolate and detect floating objects like ships, boats, buoys, or debris within the segmented sea regions.

By leveraging advanced segmentation techniques, the system not only helps in automating the monitoring process but also enhances the accuracy and reliability of naval surveillance. This automation reduces the cognitive load on human operators and supports faster, more consistent threat detection across varying environmental conditions.

The project involves training a semantic segmentation model, creating an annotated dataset with relevant maritime classes, and building an inference pipeline capable of processing both images and video frames. The end result is a practical solution that can overlay segmentation masks and bounding boxes on live or recorded surveillance footage—providing clear, real-time insights for defense and maritime monitoring applications.

Problem Statement

Maritime surveillance is essential for safeguarding coastal areas, monitoring ship traffic, and detecting potential threats. However, the visual data collected from surveillance systems—typically in the form of images or video streams—is often complex and cluttered due to dynamic lighting, water reflections, and environmental factors like haze or waves. Manually analyzing this data to detect regions of interest (land, sky, sea) and identifying floating objects (like boats, ships, debris, and buoys) is both time-consuming and prone to human error.

The primary challenge this project addresses is the automatic segmentation and detection of maritime scenes for enhanced situational awareness. This includes two major tasks:

    Semantic Segmentation: Accurately separating land, sky, and sea regions in each frame to provide a meaningful spatial understanding of the environment.

    Object Detection: Identifying and localizing floating objects specifically within the segmented sea regions.

To tackle these tasks, this project employs a combination of state-of-the-art deep learning models:

    For semantic segmentation, two models are explored:

        WaRS (Water and Region Segmentation): A model tailored for maritime environments that helps in distinguishing between land, sky, and sea with high precision.

        SAM2 (Segment Anything Model - version 2): A generalized segmentation model that offers strong performance across diverse visual inputs, further refined for maritime use in this project.

    For object detection, two complementary approaches are used:

        YOLO (You Only Look Once): A real-time object detection model known for its speed and efficiency in identifying objects in complex scenes.

        Detection Transformer (DETR): A transformer-based model that offers high accuracy by modeling object relationships and spatial dependencies, making it suitable for detecting small or partially occluded objects in sea regions.

By integrating these models, the project aims to create a robust pipeline capable of real-time segmentation and detection, reducing human effort while increasing the accuracy and efficiency of maritime surveillance systems.


WaRS Model (Water and Region Segmentation)

The WaRS (Water and Region Segmentation) model is a deep learning architecture specifically designed to perform semantic segmentation in maritime environments. It focuses on accurately separating key regions such as sea (water), land, and sky, which are fundamental for applications like naval surveillance, search and rescue, and environmental monitoring.

Traditional segmentation models often struggle with maritime imagery due to unique challenges like wave patterns, sunlight reflections, object occlusion, and the highly dynamic nature of water surfaces. The WaRS model addresses these limitations through a tailored network architecture and training strategy suited for marine scenes.
Key Features and Architecture:

    Encoder-Decoder Structure:
    WaRS follows the popular encoder-decoder paradigm similar to U-Net or DeepLab variants. The encoder extracts hierarchical features from the input image, capturing both low-level textures and high-level semantic information. The decoder then reconstructs the segmentation map by progressively upsampling and refining the features to the original resolution.

    Multi-Scale Contextual Learning:
    Maritime scenes contain objects and textures of varying sizes—from small buoys to large ships and vast sea backgrounds. WaRS incorporates multi-scale feature fusion layers, allowing the network to understand both global context and fine-grained details. This helps in improving boundary accuracy between land, sea, and sky.

    Attention Mechanisms:
    To further enhance its focus on relevant regions (especially dynamic water areas), the WaRS model integrates attention modules that help the network concentrate on spatial regions that are semantically significant while ignoring irrelevant background noise.

    Custom Loss Functions:
    The model is often trained using a combination of cross-entropy loss and boundary-aware losses to penalize misclassification at edges, which are common in maritime segmentation tasks. This improves the clarity of separation between land, sea, and sky regions.

    Lightweight and Efficient:
    Unlike very heavy segmentation models, WaRS is designed to be relatively lightweight, making it suitable for real-time or near real-time inference on embedded systems or naval platforms with limited compute resources.

Performance in Maritime Scenes:

WaRS performs robustly across varied maritime conditions such as:

    Daytime and nighttime imagery

    Rough and calm sea states

    Scenes with partial occlusions or floating objects

    Satellite and drone-based perspectives

It has shown strong generalization on datasets involving naval surveillance, with high pixel-level accuracy and reliable region separation, especially in differentiating the sea region from other classes. This is particularly important because floating object detection (e.g., using YOLO or DETR) is only meaningful when restricted to the segmented sea area.
Integration in This Project:

In this project, WaRS was used as a core semantic segmentation module to:

    Classify each pixel into one of the classes: Land, Sky, or Sea.

    Create a reliable binary mask for the sea region, which is then passed as a constraint to object detection models (YOLO/DETR) to identify and localize floating objects only within the water region.

    Generate clean visual overlays to assist operators in maritime monitoring tasks.

By using WaRS, the system achieves better segmentation accuracy and region-specific detection, significantly improving the relevance and interpretability of the surveillance output.

SAM2 Model (Segment Anything Model - Version 2)

The Segment Anything Model (SAM), developed by Meta AI, represents a new direction in vision AI—designed to generalize across a wide range of segmentation tasks without needing task-specific fine-tuning. The SAM2 version builds upon the success of the original SAM model, introducing architectural improvements, better prompt handling, and higher accuracy across complex visual domains like maritime scenes.

SAM2 is part of a broader vision to create a foundation model for segmentation, similar to how models like BERT or GPT serve as general-purpose foundations for natural language tasks.
Purpose and Strength in Maritime Segmentation:

In maritime surveillance, images are often challenging due to water reflections, dynamic waves, varying lighting, and partial occlusions. Traditional segmentation models require extensive annotated data and task-specific training. SAM2 bypasses this need by supporting promptable segmentation, meaning it can generate segmentation masks using only lightweight guidance such as points, boxes, or masks—making it highly adaptable even to unseen objects or noisy environments.
Core Architecture and Components:

SAM2 comprises three main components:

    Image Encoder:

        A Vision Transformer (ViT) pre-trained on a massive dataset, enabling the model to understand diverse visual concepts and generalize across domains.

        In SAM2, the image encoder is enhanced for better spatial resolution and long-range dependency modeling, which is crucial for maritime scenes with wide, open areas like the sea or sky.

    Prompt Encoder:

        Accepts various prompts such as:

            Points (positive or negative)

            Bounding boxes

            Predefined masks

        These prompts help guide the model to focus on specific regions, like the sea, a ship, or a floating object.

        In SAM2, the prompt encoder is optimized for faster inference and improved spatial alignment.

    Mask Decoder:

        Combines the image embeddings and the prompt embeddings to generate high-quality segmentation masks.

        The mask decoder in SAM2 produces multiple candidate masks with confidence scores, enabling selection of the most accurate output.

        It includes attention modules and multi-scale reasoning to enhance mask precision—especially helpful in cluttered maritime images.

Key Features of SAM2:

    Generalization to Unseen Objects:
    SAM2 doesn’t require class-specific training. It can segment new object types based on user prompts, making it ideal for maritime applications where new floating objects or unknown debris may appear.

    Promptable Segmentation:
    Unlike traditional models that require full scene understanding, SAM2 can segment a specific region (like the sea) using a point or bounding box as input—making it more flexible in real-world surveillance systems.

    Multi-Mask Output with Ranking:
    For a single prompt, SAM2 can return multiple plausible segmentation masks and rank them based on confidence, helping reduce false positives in noisy scenes.

    Scalability and Transferability:
    Since it is trained on billions of masks across diverse images, SAM2 easily adapts to complex maritime visuals without additional retraining.

Integration in This Project:

In this project, SAM2 was used alongside the WaRS model for the segmentation of maritime scenes, specifically for:

    Assisting in segmenting sea, sky, and land regions, especially when boundary conditions are unclear or occluded.

    Generating refined masks for complex images where WaRS might underperform due to limited training examples.

    Providing an interactive or automated way to segment floating objects based on user-defined points or bounding boxes, before passing the data to detection models like YOLO or DETR.

Advantages Over Traditional Models in Maritime Use:
Feature	Traditional Models	SAM2
Needs Retraining	Yes	No
Works with Prompts	No	Yes (points, boxes, masks)
Object-Agnostic	No (needs classes)	Yes
Supports Interactive Use	No	Yes
Handles Complex Scenes	Moderate	High
Conclusion:

SAM2 brings unprecedented flexibility and robustness to the segmentation task. In the context of this maritime surveillance project, it complements the WaRS model by offering prompt-based adaptability, fine-grained segmentation, and generalization to unknown objects, significantly enhancing the system's overall performance and reliability.



YOLOv5 (You Only Look Once – Version 5)

YOLOv5 is one of the most widely adopted object detection models in the deep learning community, known for its speed, accuracy, and efficiency. It belongs to the YOLO (You Only Look Once) family of models, which revolutionized object detection by introducing a single-stage detection architecture capable of real-time performance.

YOLOv5 was developed and maintained by Ultralytics, and unlike its predecessors (YOLOv1–v4), it is implemented in PyTorch, making it more accessible and easier to customize.
Why YOLOv5 for Maritime Detection?

In this project, YOLOv5 is used for detecting floating objects (such as ships, boats, debris, and buoys) within the segmented sea region. This task is challenging due to:

    Variations in object size and shape

    Occlusions and water reflections

    Low-contrast or blurry imagery from naval sensors

YOLOv5 was selected because it performs exceptionally well in detecting objects even in such challenging scenarios, without requiring large inference times.
Key Features of YOLOv5:

    Single-Stage Detector:

        YOLOv5 divides the input image into a grid and directly predicts bounding boxes and class probabilities in one forward pass.

        This makes it much faster than two-stage detectors like Faster R-CNN.

    Backbone and Neck:

        The backbone (CSP-Darknet) extracts features from the input image.

        The neck (PANet/FPN) helps in aggregating multi-scale features, allowing the model to detect both small and large objects.

    Head:

        The head layer performs final object classification and bounding box regression using anchor boxes.

        It outputs results at three different scales, which improves detection across object sizes—a key advantage when dealing with both small debris and large ships.

    Lightweight and Efficient:

        Comes in multiple sizes (YOLOv5s, YOLOv5m, YOLOv5l, YOLOv5x) that trade off between speed and accuracy.

        The YOLOv5s version is very fast and suitable for edge devices, while YOLOv5x provides higher accuracy for complex scenes.

    Auto Learning Bounding Boxes:

        During training, YOLOv5 automatically adjusts anchor boxes to better fit the dataset, improving detection accuracy without manual tuning.

YOLOv5 in This Project:

In this maritime surveillance project, YOLOv5 is applied after the segmentation step (from WaRS or SAM2), specifically on the sea region, to:

    Detect and localize floating objects such as:

        Ships and boats

        Buoys

        Debris

    Generate bounding boxes for each object with class labels and confidence scores

    Support real-time or near real-time inference on image frames or video streams

This two-step approach (segment first, then detect) ensures that detection is focused and accurate, as it avoids false positives in non-water regions.
Advantages of YOLOv5 in Maritime Scenes:
Criteria	Performance
Speed	Real-time (FPS > 30 on GPU)
Accuracy	High (especially with YOLOv5m/l/x)
Small object detection	Good (due to multi-scale outputs)
Model flexibility	High (easily switch model size)
Ease of use	High (PyTorch-based, well-documented)
Conclusion:

YOLOv5 brings an ideal balance of speed, accuracy, and deployability, making it a perfect fit for naval surveillance tasks where fast and reliable detection of floating objects is critical. Combined with semantic segmentation models (WaRS or SAM2), YOLOv5 forms the detection backbone of a powerful multi-stage maritime monitoring system.

Detection Transformer (DETR)

The Detection Transformer (DETR) is a groundbreaking object detection model introduced by Facebook AI Research (FAIR), which combines the power of transformers—originally used in natural language processing—with convolutional visual feature extraction. DETR reformulates the traditional object detection pipeline as a direct set prediction problem, offering a cleaner, end-to-end approach to detecting objects in images.

Unlike traditional object detectors like YOLO or Faster R-CNN that rely on anchor boxes, region proposals, or non-maximum suppression (NMS), DETR replaces these components with a transformer-based attention mechanism and bipartite matching.
Why DETR for Maritime Object Detection?

In maritime environments, object detection is particularly challenging due to:

    Small and sparsely located objects (e.g., buoys, debris)

    Cluttered and reflective sea surfaces

    Occlusions and irregular shapes

DETR excels in such cases because it models global relationships between objects and their surroundings using self-attention. This makes it particularly effective in identifying subtle or overlapping objects that may be missed by anchor-based detectors like YOLO.
Architecture Overview:

DETR is composed of the following main components:

    Backbone (e.g., ResNet):

        Extracts feature maps from the input image.

        These features are then flattened and passed to the transformer.

    Transformer Encoder-Decoder:

        The encoder processes the visual features and captures global context.

        The decoder takes in a fixed number of object queries (learnable embeddings) and generates predictions for objects present in the image.

        Each query attends to the image and decides if it corresponds to an actual object or should remain blank (no-object class).

    Prediction Heads:

        For each object query, DETR directly outputs:

            A bounding box (center x, center y, width, height)

            A class label

        These predictions are compared against ground truth using a Hungarian bipartite matching loss to ensure one-to-one correspondence between predictions and targets.

Key Features of DETR:
Feature	Description
Anchor-Free Detection	No need for predefined anchor boxes or region proposals
Global Context Understanding	Self-attention allows DETR to model relationships between objects and their background
Set Prediction	Predicts a fixed-size set of objects with one-to-one matching
Simplified Pipeline	End-to-end trainable without custom components like NMS or RPN
DETR in This Project:

In this maritime surveillance project, DETR is used alongside YOLOv5 to detect floating objects within the segmented sea region. Its role includes:

    Accurately identifying and localizing boats, ships, or debris—even when they are small, partially submerged, or close together.

    Offering more reliable detections in cases where overlapping objects confuse traditional models.

    Serving as a complementary model to YOLOv5, especially for improving detection quality in difficult conditions.

YOLOv5 vs. DETR: Complementary Roles
Aspect	YOLOv5	DETR
Detection Speed	Very fast (real-time)	Slower (but improving)
Architecture Type	CNN-based (anchor-based)	Transformer-based (anchor-free)
Small Object Handling	Good (multi-scale outputs)	Better with complex scenes
Global Context	Limited	Strong (via self-attention)
Training Complexity	Easier	Requires longer training time
Conclusion:

DETR brings transformer-level reasoning and global context awareness into the object detection domain, making it especially useful in complex maritime scenes where traditional CNN-based models may struggle. When combined with segmentation models (like WaRS or SAM2) and other detectors (like YOLOv5), DETR helps build a robust and intelligent surveillance system capable of handling real-world challenges at sea.

Reducing False Positives Using SAM2 Water Mask Integration

While YOLOv5 is a highly efficient and fast object detection model, it can sometimes produce false positives, especially in complex or cluttered maritime backgrounds. This is particularly common in images with overlapping textures (like waves and reflections), similar-looking structures (like shorelines or boats near the horizon), or occluded objects.

In such scenarios, YOLOv5 may generate bounding boxes for objects in irrelevant areas—such as on land, in the sky, or on reflective water surfaces where no real object exists. These false detections can significantly reduce the reliability of the system, especially in a real-world surveillance context where accuracy is critical.

To overcome this, we introduce a post-processing filter based on semantic segmentation masks generated by the SAM2 model.
Integration Strategy: YOLOv5 + SAM2

The proposed approach combines the strengths of both models:

    SAM2 provides a high-quality segmentation mask that separates sea (water) regions from land and sky.

    YOLOv5 detects all possible objects in the scene, regardless of their location.

After detection, a geometric filtering step is applied where:

    Only those bounding boxes predicted by YOLOv5 that intersect or overlap significantly with the sea region in the SAM2-generated segmentation mask are retained.

All other detections—especially those located on land or in the sky—are discarded as false positives.
Advantages of This Fusion Approach:

    ✅ Reduces noise from background structures like docks, rocks, or land vehicles.

    ✅ Ensures object detection is context-aware—only objects truly floating on water are considered.

    ✅ Improves overall precision, particularly in complex maritime scenes.

    ✅ Makes the pipeline more reliable for downstream tasks such as threat detection or object tracking.

Illustrative Example:

Imagine a surveillance image where a docked ship and some rocks appear on the coastline. YOLOv5 may detect them all as floating objects. However, when applying the SAM2 segmentation mask, only those bounding boxes touching the sea mask will be retained, eliminating detections that fall on land or other irrelevant regions.
Conclusion:

This mask-based filtering mechanism introduces a simple yet effective constraint that aligns detection with environmental context. By integrating YOLOv5 with SAM2’s segmentation output, the system achieves higher precision and more meaningful object detections in maritime environments.

Dataset Description: Singapore Maritime Dataset (SMD)

The Singapore Maritime Dataset (SMD) is a publicly available dataset curated specifically for research in maritime object detection and scene understanding. Developed by the Centre for Remote Imaging, Sensing and Processing (CRISP) at the National University of Singapore, the SMD serves as a valuable benchmark for evaluating computer vision models in real-world naval environments.

This dataset is widely used in projects related to autonomous vessel navigation, maritime surveillance, and floating object detection due to its rich variety of scenes and annotated object classes.
Key Features of the SMD:

    Realistic Maritime Scenarios:

        The dataset consists of videos and image frames captured from both visible (VIS) and infrared (IR) sensors mounted on surveillance systems or autonomous vessels.

        Scenes include ports, open sea, coastal waters, and cluttered shorelines, reflecting real operational conditions.

    Variety of Object Classes:

        The SMD covers multiple floating object types such as:

            Ships

            Boats

            Buoys

            Debris

            Other unknown objects

        Each object is annotated with a bounding box, and sometimes includes motion or distance labels (depending on the subset used).

    Video and Image Annotations:

        The dataset provides frame-wise annotations, often stored in MATLAB (.mat) format, containing structured fields like:

            Bounding box coordinates (BB)

            Object type

            Motion state

            Distance category

    Challenging Environmental Conditions:

        Includes diverse lighting (day/night), weather conditions, and occlusion scenarios, making it ideal for testing model robustness.

        Also includes cluttered backgrounds, reflections, and small, low-contrast objects—perfect for stress-testing detection algorithms like YOLO and DETR.

    Structured Folders:

        Typically split into training and testing subsets with organized folders:

            /image/ or /frames/ folder containing extracted RGB images

            /groundtruth/ or /mat/ folder containing object annotations

Usage in This Project:

In this project, the RGB frames extracted from the SMD's video sequences were used as input for both:

    Semantic segmentation using WaRS and SAM2, and

    Object detection using YOLOv5 and DETR.

The ground truth annotations provided in .mat format were parsed and converted to standard formats (e.g., COCO or CSV) for model training and evaluation. The dataset's real-world variability helped train models that generalize well to various maritime environments.
Conclusion:

The Singapore Maritime Dataset provides a comprehensive and realistic platform for training and validating vision-based maritime systems. Its complexity and high-quality annotations make it a benchmark choice for researchers working in naval surveillance, autonomous maritime navigation, and object detection in oceanic environments.


Dataset Description: LaRS Dataset (Light-weight Aerial Maritime Surveillance)

The LaRS (Light-weight Aerial Maritime Surveillance) dataset is a publicly available dataset specifically designed for semantic segmentation in maritime environments. It is curated to support the development and evaluation of deep learning models for scene understanding in the context of aerial and maritime surveillance.

LaRS focuses on classifying key visual components in maritime images such as sky, land, and water (sea)—which are fundamental for high-level tasks like object detection, tracking, and anomaly detection in naval or coastal monitoring systems.
Key Features of the LaRS Dataset:

    Semantic Segmentation Labels:

        Each image in the dataset is paired with a pixel-level semantic mask, where each pixel is annotated as:

            Sky

            Land

            Water (Sea)

        This labeling enables training and evaluation of multi-class segmentation models such as U-Net, WaRS, or SAM2.

    Real-World Maritime Scenes:

        Images are collected from aerial surveillance platforms, providing a top-down or oblique view of coastal regions, harbors, and open sea.

        The dataset reflects operational environments with variable lighting, complex shorelines, and distant horizons.

    Diverse Visual Conditions:

        Includes scenes with:

            Clear sky and cloudy conditions

            Calm and rough sea surfaces

            Natural and man-made land features like docks, ports, and coastlines

    Compact and Lightweight:

        LaRS is intentionally designed to be lightweight (small in size) for rapid prototyping and experimentation.

        It is particularly useful for academic research, benchmarking, or testing smaller models on maritime segmentation tasks.

Usage in This Project:

In this project, the LaRS dataset was used to train and evaluate semantic segmentation models, particularly:

    WaRS: The dataset serves as a training source to teach the model how to accurately distinguish between sea, sky, and land regions.

    SAM2: The annotated masks were used to benchmark the model’s general segmentation capabilities in maritime contexts.

The dataset’s high-quality pixel-level annotations and real-world complexity made it a strong foundation for developing the segmentation module of the maritime monitoring pipeline.
Advantages of LaRS Dataset:
Attribute	Description
Pixel-level Accuracy	Provides precise labels for sky, sea, and land
Lightweight	Easy to download, process, and experiment with
High Relevance	Specifically tailored for maritime segmentation
Complementary	Works well alongside detection datasets like SMD
Conclusion:

The LaRS dataset plays a key role in enabling robust semantic segmentation in maritime surveillance systems. By offering high-quality annotations in real-world aerial scenes, it allows models to learn meaningful class boundaries—especially critical for sea region isolation, which directly supports downstream tasks like object detection using YOLOv5 or DETR. Its simplicity and effectiveness make it an ideal companion to more complex datasets like the Singapore Maritime Dataset (SMD).

WaSR-T: Temporal-Aware Maritime Obstacle Segmentation
Overview

The WaSR-T model is an advanced deep learning architecture designed specifically for maritime obstacle segmentation, extending the capabilities of the WaSR (Water and Sky Regions) network by introducing a temporal context. Unlike traditional single-frame segmentation methods, WaSR-T uses sequences of consecutive video frames to improve its understanding of the scene, particularly in challenging scenarios involving water reflections and sun glitter.

Maritime environments often present deceptive visual cues due to reflective water surfaces, which make it difficult to distinguish between true obstacles and reflections or lighting artifacts. WaSR-T addresses this by analyzing how pixel patterns evolve over time—enabling it to differentiate moving or static obstacles from dynamic water textures.
Motivation

The rise of Unmanned Surface Vehicles (USVs) in tasks like port surveillance, environmental monitoring, and autonomous cargo navigation has made robust obstacle detection crucial. While low-power and rich-informational cameras are ideal for these vehicles, models trained for land-based applications (e.g., autonomous cars) often fail in maritime settings.

Traditional models such as WaSR process images independently, which makes them vulnerable to false positives caused by reflections or lighting artifacts. These false alarms lead to unnecessary slowdowns and misinterpretations in autonomous navigation. WaSR-T was developed to reduce these errors by analyzing temporal patterns across multiple frames.
Key Innovations of WaSR-T

    Temporal Context Module (TCM):

        WaSR-T introduces a Temporal Context Module that processes a short sequence of frames (e.g., 5 preceding frames) to capture how pixel patterns change over time.

        Reflections typically deform over time due to water motion, whereas true obstacles remain relatively stable.

        This insight is leveraged by applying 3D convolutions across stacked feature maps from consecutive frames.

    Shared Encoder Architecture:

        All frames are passed through a shared ResNet-101 encoder, generating spatial feature maps for each.

        These maps are then reduced in dimensionality and fused using the Temporal Context Module before being passed to the decoder.

    Improved Decoder Integration:

        The decoder uses skip connections and multi-scale feature fusion, similar to the original WaSR.

        The fused temporal features are decoded to generate pixel-wise segmentation of the target frame into:

            Water

            Sky

            Obstacle

Dataset and Training

    Base Dataset: MaSTr1325

        Contains 1,325 fully segmented maritime images (with pixel-wise labels for water, sky, and obstacles).

    Extended Dataset: MaSTr1478

        Created for training WaSR-T, it adds 153 challenging frames with their temporal sequences (5 preceding frames each).

        New frames include difficult scenarios like:

            Sun glitter

            Floating object reflections

            Motion blur

        Balanced sampling between standard and challenging conditions ensures model robustness.

Performance Highlights

    On the MODS benchmark, WaSR-T:

        Reduces false positives by 30% compared to WaSR.

        Maintains efficient inference by caching feature maps and only computing new embeddings for incoming frames.

        Shows superior performance especially in the 15-meter critical zone around the USV, which is vital for collision avoidance.

Why WaSR-T is Important for This Project

In this maritime perception project, WaSR-T is used to segment the sea, sky, and obstacles with enhanced reliability. Its temporal awareness allows it to:

    Filter out dynamic water reflections that YOLO or DETR might falsely detect as floating objects.

    Provide a stable and consistent sea mask, which is crucial for post-processing steps like:

        Filtering YOLO detections

        Assigning priority to obstacles in the danger zone

Conclusion

WaSR-T represents a significant leap in maritime perception by bringing temporal intelligence into semantic segmentation. Its ability to reduce false positives from water reflections and glitter, while maintaining real-time processing speed, makes it an ideal fit for autonomous maritime systems and USV-based applications. When combined with object detection models, WaSR-T plays a foundational role in ensuring navigational safety and scene understanding in complex maritime environments.


Technologies Used

To successfully implement and evaluate the segmentation and detection models for maritime obstacle recognition, a wide range of development tools, programming frameworks, and supporting technologies were employed. These technologies facilitated efficient model development, training, testing, and result visualization throughout the project lifecycle.
1. Programming Language

    Python 3.8+

        Used as the primary programming language due to its simplicity, large scientific ecosystem, and excellent support for deep learning and computer vision libraries.

2. Deep Learning Frameworks

    PyTorch

        Core framework used for building and training neural networks such as WaSR-T, YOLOv5, and Detection Transformer (DETR).

        Chosen for its dynamic computation graph, flexibility, and wide adoption in research.

    Torchvision

        Used for standard model architectures, image transformations, and datasets.

3. Image Processing Libraries

    OpenCV

        Used for image manipulation tasks like resizing, frame extraction, video-to-image conversion, and mask visualization.

    PIL (Python Imaging Library) / Pillow

        Used for loading and saving images, and performing basic preprocessing.

4. Scientific Computing

    NumPy

        Used for handling matrix operations, mask manipulation, and mathematical computations.

    SciPy / MATLAB

        Used for handling and parsing .mat annotation files from the Singapore Maritime Dataset.

5. Annotation and Visualization Tools

    Matplotlib / Seaborn

        Used for visualizing model outputs, segmentation masks, performance metrics, and training curves.

    Labelme / CVAT (Optional)

        Considered for any custom labeling or reviewing annotations.

6. Pretrained Model Repositories

    Ultralytics YOLOv5 GitHub Repo

        Provided pre-trained weights and modular codebase for training, inference, and evaluation of the YOLOv5 model.

    DETR (Facebook AI GitHub)

        Used as a reference for implementing the Detection Transformer.

7. Dataset Handling and Conversion

    Scikit-learn

        Used for train-validation split, and performance metric calculations like F1-score, precision, and recall.

    Custom Python Scripts

        Used for converting .mat files to COCO or CSV format.

        Used to merge SAM2 segment masks with detection outputs.

8. GPU and Acceleration

    NVIDIA CUDA / cuDNN

        Leveraged for GPU acceleration during training and inference.

    Google Colab / Kaggle Notebooks

        Used for prototyping and training models in GPU-enabled cloud environments.

9. Version Control

    Git and GitHub

        Used for code versioning, collaboration, and experiment tracking.

Conclusion

By integrating these technologies, the project maintained a balance between flexibility, performance, and research reproducibility. The synergy between deep learning frameworks like PyTorch, tools like OpenCV, and annotation handling via custom scripts enabled an efficient workflow for developing and deploying maritime segmentation and detection models.


Implementation

The implementation phase of this project involved building a robust maritime perception pipeline that integrates semantic segmentation and object detection models for improved obstacle awareness in marine environments. The key focus was on designing a system that could accurately detect floating objects by isolating sea regions and eliminating false positives caused by complex backgrounds like land, reflections, and sun glitter.
1. Data Preprocessing and Annotation Parsing

    Frame Extraction:

        Video sequences from the Singapore Maritime Dataset (SMD) were converted into individual RGB frames using OpenCV.

    Annotation Conversion:

        .mat files containing bounding box annotations were parsed and converted into COCO-style JSON format to train object detection models (YOLOv5 and DETR).

    Segmentation Masks:

        For the LaRS and MaSTr1478 datasets, segmentation masks were resized and normalized.

        Semantic masks were categorized into three classes: Sky, Water, and Obstacle.

2. Semantic Segmentation Models

    WaSR-T Implementation:

        WaSR-T was implemented using PyTorch, with a ResNet-101 encoder and a custom-designed Temporal Context Module (TCM) using 3D convolutions.

        For each target frame, the previous 5 frames were stacked to generate spatiotemporal embeddings.

        Skip connections and ASPP (Atrous Spatial Pyramid Pooling) were used in the decoder to refine segmentation results.

    SAM2 Model:

        The SAM2 (Segment Anything Model) was used to generate high-accuracy water masks for complex maritime scenes.

        These masks were used to filter detection outputs and eliminate bounding boxes not overlapping with water.

3. Object Detection Models

    YOLOv5:

        Trained on COCO-style SMD annotations.

        Provided fast detection of floating objects like boats, debris, and buoys.

        Post-processing was applied to keep only detections overlapping with sea region (via SAM2 masks) to reduce false positives.

    DETR (Detection Transformer):

        Implemented using the official PyTorch version from Facebook AI.

        Trained to predict object classes and bounding boxes using transformer-based attention.

        Served as a complementary model to YOLOv5, improving detection robustness in cluttered scenes.

4. Integration Workflow

The final pipeline integrates all components in the following order:

    Input Video Frame ⟶

    WaSR-T or SAM2 Segmentation ⟶

    Sea Region Mask Extraction ⟶

    YOLOv5 / DETR Detection ⟶

    Mask-Based Filtering: Keep only detections within sea mask

    Final Output: Display segmented frame with filtered bounding boxes

5. Post-Processing and Output Generation

    Bounding boxes overlapping significantly (IoU > threshold) with sea masks were retained.

    Overlayed segmentation and detection outputs were visualized for evaluation.

    Precision, Recall, and F1-score were computed to assess model performance.

Conclusion

The implementation successfully combines semantic segmentation and object detection in a modular, real-time-ready pipeline. The integration of temporal awareness (WaSR-T) and mask-guided filtering enhances robustness in complex maritime scenarios, demonstrating potential for real-world deployment in autonomous marine systems.

Project Requirements and Specifications

This section outlines the hardware and software environment required to develop, train, and test the segmentation and detection models used in this maritime perception project. Due to the computational demands of deep learning models—especially those involving temporal modules and large image sequences—a high-performance setup was necessary.
1. Hardware Requirements
Component	Specification
GPU	NVIDIA RTX A1000 (24 GB VRAM)
CPU	Intel Xeon / AMD Ryzen 7 or higher
RAM	Minimum 32 GB DDR4
Storage	At least 100 GB free SSD space for datasets and model outputs
Display	High-resolution display for output visualization
Power	Compatible high-wattage power supply for GPU computation

    The NVIDIA RTX A1000 GPU with 24 GB VRAM was crucial for training large models like WaSR-T and DETR, especially when using batch sizes that include multiple temporal frames.

    The large memory also helped accelerate inference and reduce training time significantly.

2. Software Requirements
Software	Version / Details
Operating System	Ubuntu 20.04 / Windows 10 / Linux Kernel 5+
Python	3.8 or higher
PyTorch	1.13 or higher
Torchvision	Compatible version with PyTorch
CUDA Toolkit	CUDA 11.3 or higher
cuDNN	v8 or above
OpenCV	4.x
Matplotlib / Seaborn	For visualization
Jupyter / Colab / VS Code	For development and debugging
Git	Version control

    PyTorch with CUDA support was used to accelerate deep learning training and inference on GPU.

    Jupyter notebooks or VS Code were used for experimentation, visualization, and debugging.

3. Dataset Requirements
Dataset	Description	Size (Approx.)
Singapore Maritime Dataset (SMD)	Object detection dataset with .mat annotations and RGB video frames	~15–20 GB
LaRS Dataset	Segmentation dataset with sky, land, sea masks	~1–2 GB
MaSTr1325 / MaSTr1478	High-resolution segmented maritime frames with temporal context	~5–6 GB

Large disk space was needed to store both original and preprocessed versions of these datasets (e.g., resized frames, masks, JSON annotations).


